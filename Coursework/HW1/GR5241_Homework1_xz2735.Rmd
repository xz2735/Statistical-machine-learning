---
title: "GR5241_Homework1_xz2735"
author: "Xiaofan Zhang"
date: "1/28/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1.
### 1&2&3.
```{r}
curve(dexp(x,rate=1),from=0,to=4,lty=1)
curve(dexp(x,rate=2),from=0,to=4,add=TRUE,col="red",lty=2)
legend("topright",legend=c("theta=1","theta=2"),col = c("black","red"),lty=1:2 )
points(1,exp(-1))
points(2,exp(-2))
points(4,exp(-4))
```
So the higher rate decreases the likelihood of of sample that is bigger than approximate 1.7.

### Question 1
As $$f_{n}(x|\theta)=\prod_{i=1}^{n}\theta e^{-\theta x_{i}}=\theta^{n}e^{-\theta \sum_{i=1}^{n} x_{i}}$$
The posterior is $$\xi(\theta|x)\propto \theta^{n}e^{-\theta \sum_{i=1}^{n} x_{i}} \cdot \theta^{\alpha-1} e^{-\beta \theta}=\theta^{n+\alpha-1} e^{-(\beta+\sum_{i=1}^{n} x_{i})\theta}$$
Thereore, $\theta|X_{n} = x \sim Gamma(n+\alpha,\beta+\sum_{i=1}^{n} x_{i})$.

### Question 2
#### a.
As $$\prod(\theta|X_{1:n}) = x \sim Gamma(n+\alpha,\beta+\sum_{i=1}^{n} x_{i})$$ obtained above
so $$\widetilde q(\theta)=\prod(\theta|X_{1:n-1}) = x \sim Gamma(n-1+\alpha,\beta+\sum_{i=1}^{n-1} x_{i})$$.
By Induction, $$\xi(\theta|X)\propto \theta^{n}e^{-\theta \sum_{i=1}^{n} x_{i}} \cdot \theta^{\alpha-1} e^{-\beta \theta} = \theta^{n+\alpha-1} e^{-(\beta+\sum_{i=1}^{n} x_{i})\theta}=\theta^{n-1+\alpha-1} e^{-(\beta+\sum_{i=1}^{n-1} x_{i})\theta} \cdot \theta e^{-x_{n} \theta}=\widetilde q(\theta)\cdot \prod(\theta|X_{n})$$

#### b.
```{r}
a = rexp(256,rate=1)
curve(dgamma(x,shape=4+2,rate=0.2+sum(a[1:4])),from=0,to=4,col="red",ylim=c(0,6),lwd=1.5,ylab="density",lty=1)
curve(dgamma(x,shape=8+2,rate=0.2+sum(a[1:8])),from=0,to=4,add=TRUE,col="blue",lty=2)
curve(dgamma(x,shape=16+2,rate=0.2+sum(a[1:16])),from=0,to=4,add=TRUE,col="orange",lty=3)
curve(dgamma(x,shape=256+2,rate=0.2+sum(a)),from=0,to=4,add=TRUE,col="dark green",lty=4)
legend("topright",legend=c("n=4","n=8","n=16","n=256"),col=c("red","blue","orange","dark green"),lty = 1:4 )

```

As n increases, the scale of posterior distribution shrinks and the peak becomes larger.

### Problem 2
$T_{i}$ follows $Bernoulli(0.5)$, likelihood of $T_{i}$ is $\frac{1}{2}^n$,
Now, we know that $Y^{T_{1}}\sim Bernoulli(\pi^1)$ and $Y^{T_{2}}\sim Bernoulli(\pi^2)$. The number of patients who received treatment one is $n_{1}=\sum_{i=1}^{n}I(T_{i}=1)$, The number of patients who received other treatment is $n_{2}=n-n_{1}=n-\sum_{i=1}^{n}I(T_{i}=1)$, where $I(T_{i}=1)$ is an indicator.
Here, $$f(Y^t,T|\pi^1,\pi^2)=\prod_{i=1}^{n}(\pi^1)^{Y^1_{i}}(1-\pi^1)^{1-(Y^1_{i})}(\pi^2)^{Y^2_{i}}(1-\pi^2)^{1-(Y^2_{i})}$$
$$=(\pi^{1})^{\sum_{i=1}^{n} Y^1_{i}}(1-\pi^1)^{n_{1}-{\sum_{i=1}^{n} Y^1_{i}}}(\pi^{2})^{\sum_{i=1}^{n}Y^2_{i}}(1-\pi^2)^{n_{2}-{\sum_{i=2}^{n}Y^2_{i}}}$$
Thus, $$\xi((\pi^{1},\pi^{2})|Y^{T_{1}}_{1},...,Y^{T_{n}}_{n},T_{1},...,T_{n})\propto \frac{1}{2}^nf(Y^t,T|\pi^1,\pi^2) \cdot 1$$ $$\propto f(Y^t,T|\pi^1,\pi^2) = (\pi^{1})^{\sum_{i=1}^{n} Y^1_{i}}(1-\pi^1)^{n_{1}-{\sum_{i=1}^{n} Y^1_{i}}}(\pi^{2})^{\sum_{i=1}^{n}Y^2_{i}}(1-\pi^2)^{n_{2}-{\sum_{i=2}^{n}Y^2_{i}}}$$
So,$$\xi(\pi^{1}|Y^{T_{1}}_{1},...,Y^{T_{n}}_{n},T_{1},...,T_{n})\propto (\pi^{1})^{\sum_{i=1}^{n} Y^1_{i}}(1-\pi^1)^{n_{1}-{\sum_{i=1}^{n} Y^1_{i}}}$$,
$$\xi(\pi^{2}|Y^{T_{1}}_{1},...,Y^{T_{n}}_{n},T_{1},...,T_{n})\propto (\pi^{2})^{\sum_{i=1}^{n} Y^2_{i}}(1-\pi^2)^{n_{2}-{\sum_{i=1}^{n} Y^2_{i}}}$$,
Finally, $$\pi^{1}|Y^{T_{1}}_{1},...,Y^{T_{n}}_{n},T_{1},...,T_{n}\sim Beta(\sum_{i=1}^{n} (Y^1_{i})+1,\sum_{i=1}^{n} I(T_{i}=1)-\sum_{i=1}^{n} (Y^1_{i})+1)$$,$$ \pi^{2}|Y^{T_{1}}_{1},...,Y^{T_{n}}_{n},T_{1},...,T_{n}\sim Beta(\sum_{i=1}^{n} (Y^2_{i})+1,n-\sum_{i=1}^{n} I(T_{i}=1)-\sum_{i=1}^{n} (Y^2_{i})+1)$$.

### Problem 3

#### (a)
$$E(\bar{X})=E(\frac{\sum_{i=1}^{n}X_{i}}{n})
=\frac{1}{n}E(\sum_{i=1}^{n}X_{i})=\frac{1}{n}\sum_{i=1}^{n}E(X_{i})=\frac{1}{n}n\lambda=\lambda$$
So $\bar{X}$ isan unbiased estimator of $\lambda$.

#### (b)
$$E(T_{n}-\lambda)^2=E(T_{n}-\bar{X}+\bar{X}-\lambda)^2=E(T_{n}-\bar{X})^2+E(\bar{X}-\lambda)^2+2E(T_{n}-\bar{X})E(\bar{X}-\lambda)=E(T_{n}-\bar{X})^2+E(\bar{X}-\lambda)^2>=E(\bar{X}-\lambda)^2$$, 
as$E(\bar{X}-\lambda)=0$. When $T_{n}=\bar{X}$, the both sides are equal. 
So $\bar{X}$ is optimal unbiased estimator among all unbiased estimator.

